{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2sVJrTP_OZX",
        "outputId": "76d432c7-33e4-41c1-bffd-713bc83b835e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting miditok\n",
            "  Downloading miditok-2.0.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.4/94.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.24,>=1.19 in /usr/local/lib/python3.10/dist-packages (from miditok) (1.22.4)\n",
            "Collecting miditoolkit>=0.1.16\n",
            "  Downloading miditoolkit-0.1.16-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from miditok) (4.65.0)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mido, miditoolkit, miditok\n",
            "Successfully installed miditok-2.0.5 miditoolkit-0.1.16 mido-1.2.10\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install miditok\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzh53DJR-7n5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "import miditok\n",
        "import numpy\n",
        "import os\n",
        "import pathlib\n",
        "from tensorflow import keras\n",
        "import json\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Config, Trainer, TrainingArguments, GenerationConfig\n",
        "from miditok import OctupleMono, MIDITokenizer, MIDILike\n",
        "from miditok.constants import CHORD_MAPS\n",
        "from miditoolkit import MidiFile\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcf6tiUF-7n8"
      },
      "outputs": [],
      "source": [
        "attn_window = 10\n",
        "j = 0\n",
        "input_seq = [] \n",
        "target_seq = []\n",
        "batch_size = 16\n",
        "buffer_size = 1000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZGVkhFr_mIo",
        "outputId": "69c3b5d6-6fff-4541-ea09-034b7ccc2ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU5l3sAC-7n9",
        "outputId": "09a8180e-4a39-475c-e1f8-c29bf89fb6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch number: 0\n",
            "input: [[49, 27, 14, 3, 3, 18], [54, 27, 14, 3, 3, 18], [57, 27, 14, 3, 3, 18], [49, 27, 14, 15, 3, 18], [53, 27, 14, 15, 3, 18], [57, 27, 14, 15, 3, 18], [50, 27, 18, 27, 3, 18], [54, 27, 18, 27, 3, 18], [57, 27, 18, 27, 3, 18], [47, 27, 26, 11, 4, 18]]\n",
            "target: [[54, 27, 14, 3, 3, 18], [57, 27, 14, 3, 3, 18], [49, 27, 14, 15, 3, 18], [53, 27, 14, 15, 3, 18], [57, 27, 14, 15, 3, 18], [50, 27, 18, 27, 3, 18], [54, 27, 18, 27, 3, 18], [57, 27, 18, 27, 3, 18], [47, 27, 26, 11, 4, 18], [53, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 1\n",
            "input: [[56, 27, 26, 11, 4, 18], [54, 27, 14, 3, 5, 18], [57, 27, 14, 3, 5, 18], [61, 27, 14, 3, 5, 18], [53, 27, 14, 15, 5, 18], [57, 27, 14, 15, 5, 18], [61, 27, 14, 15, 5, 18], [54, 27, 18, 27, 5, 18], [57, 27, 18, 27, 5, 18], [62, 27, 18, 27, 5, 18]]\n",
            "target: [[54, 27, 14, 3, 5, 18], [57, 27, 14, 3, 5, 18], [61, 27, 14, 3, 5, 18], [53, 27, 14, 15, 5, 18], [57, 27, 14, 15, 5, 18], [61, 27, 14, 15, 5, 18], [54, 27, 18, 27, 5, 18], [57, 27, 18, 27, 5, 18], [62, 27, 18, 27, 5, 18], [53, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 2\n",
            "input: [[50, 27, 14, 3, 3, 18], [55, 27, 14, 3, 3, 18], [58, 27, 14, 3, 3, 18], [50, 27, 14, 15, 3, 18], [54, 27, 14, 15, 3, 18], [58, 27, 14, 15, 3, 18], [51, 27, 18, 27, 3, 18], [55, 27, 18, 27, 3, 18], [58, 27, 18, 27, 3, 18], [48, 27, 26, 11, 4, 18]]\n",
            "target: [[55, 27, 14, 3, 3, 18], [58, 27, 14, 3, 3, 18], [50, 27, 14, 15, 3, 18], [54, 27, 14, 15, 3, 18], [58, 27, 14, 15, 3, 18], [51, 27, 18, 27, 3, 18], [55, 27, 18, 27, 3, 18], [58, 27, 18, 27, 3, 18], [48, 27, 26, 11, 4, 18], [54, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 3\n",
            "input: [[57, 27, 26, 11, 4, 18], [55, 27, 14, 3, 5, 18], [58, 27, 14, 3, 5, 18], [62, 27, 14, 3, 5, 18], [54, 27, 14, 15, 5, 18], [58, 27, 14, 15, 5, 18], [62, 27, 14, 15, 5, 18], [55, 27, 18, 27, 5, 18], [58, 27, 18, 27, 5, 18], [63, 27, 18, 27, 5, 18]]\n",
            "target: [[55, 27, 14, 3, 5, 18], [58, 27, 14, 3, 5, 18], [62, 27, 14, 3, 5, 18], [54, 27, 14, 15, 5, 18], [58, 27, 14, 15, 5, 18], [62, 27, 14, 15, 5, 18], [55, 27, 18, 27, 5, 18], [58, 27, 18, 27, 5, 18], [63, 27, 18, 27, 5, 18], [54, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 4\n",
            "input: [[52, 27, 14, 3, 3, 18], [57, 27, 14, 3, 3, 18], [60, 27, 14, 3, 3, 18], [52, 27, 14, 15, 3, 18], [56, 27, 14, 15, 3, 18], [60, 27, 14, 15, 3, 18], [53, 27, 18, 27, 3, 18], [57, 27, 18, 27, 3, 18], [60, 27, 18, 27, 3, 18], [50, 27, 26, 11, 4, 18]]\n",
            "target: [[57, 27, 14, 3, 3, 18], [60, 27, 14, 3, 3, 18], [52, 27, 14, 15, 3, 18], [56, 27, 14, 15, 3, 18], [60, 27, 14, 15, 3, 18], [53, 27, 18, 27, 3, 18], [57, 27, 18, 27, 3, 18], [60, 27, 18, 27, 3, 18], [50, 27, 26, 11, 4, 18], [56, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 5\n",
            "input: [[59, 27, 26, 11, 4, 18], [57, 27, 14, 3, 5, 18], [60, 27, 14, 3, 5, 18], [64, 27, 14, 3, 5, 18], [56, 27, 14, 15, 5, 18], [60, 27, 14, 15, 5, 18], [64, 27, 14, 15, 5, 18], [57, 27, 18, 27, 5, 18], [60, 27, 18, 27, 5, 18], [65, 27, 18, 27, 5, 18]]\n",
            "target: [[57, 27, 14, 3, 5, 18], [60, 27, 14, 3, 5, 18], [64, 27, 14, 3, 5, 18], [56, 27, 14, 15, 5, 18], [60, 27, 14, 15, 5, 18], [64, 27, 14, 15, 5, 18], [57, 27, 18, 27, 5, 18], [60, 27, 18, 27, 5, 18], [65, 27, 18, 27, 5, 18], [56, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 6\n",
            "input: [[51, 27, 14, 3, 3, 18], [56, 27, 14, 3, 3, 18], [59, 27, 14, 3, 3, 18], [51, 27, 14, 15, 3, 18], [55, 27, 14, 15, 3, 18], [59, 27, 14, 15, 3, 18], [52, 27, 18, 27, 3, 18], [56, 27, 18, 27, 3, 18], [59, 27, 18, 27, 3, 18], [49, 27, 26, 11, 4, 18]]\n",
            "target: [[56, 27, 14, 3, 3, 18], [59, 27, 14, 3, 3, 18], [51, 27, 14, 15, 3, 18], [55, 27, 14, 15, 3, 18], [59, 27, 14, 15, 3, 18], [52, 27, 18, 27, 3, 18], [56, 27, 18, 27, 3, 18], [59, 27, 18, 27, 3, 18], [49, 27, 26, 11, 4, 18], [55, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 7\n",
            "input: [[58, 27, 26, 11, 4, 18], [56, 27, 14, 3, 5, 18], [59, 27, 14, 3, 5, 18], [63, 27, 14, 3, 5, 18], [55, 27, 14, 15, 5, 18], [59, 27, 14, 15, 5, 18], [63, 27, 14, 15, 5, 18], [56, 27, 18, 27, 5, 18], [59, 27, 18, 27, 5, 18], [64, 27, 18, 27, 5, 18]]\n",
            "target: [[56, 27, 14, 3, 5, 18], [59, 27, 14, 3, 5, 18], [63, 27, 14, 3, 5, 18], [55, 27, 14, 15, 5, 18], [59, 27, 14, 15, 5, 18], [63, 27, 14, 15, 5, 18], [56, 27, 18, 27, 5, 18], [59, 27, 18, 27, 5, 18], [64, 27, 18, 27, 5, 18], [55, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 8\n",
            "input: [[53, 27, 14, 3, 3, 18], [58, 27, 14, 3, 3, 18], [61, 27, 14, 3, 3, 18], [53, 27, 14, 15, 3, 18], [57, 27, 14, 15, 3, 18], [61, 27, 14, 15, 3, 18], [54, 27, 18, 27, 3, 18], [58, 27, 18, 27, 3, 18], [61, 27, 18, 27, 3, 18], [51, 27, 26, 11, 4, 18]]\n",
            "target: [[58, 27, 14, 3, 3, 18], [61, 27, 14, 3, 3, 18], [53, 27, 14, 15, 3, 18], [57, 27, 14, 15, 3, 18], [61, 27, 14, 15, 3, 18], [54, 27, 18, 27, 3, 18], [58, 27, 18, 27, 3, 18], [61, 27, 18, 27, 3, 18], [51, 27, 26, 11, 4, 18], [57, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 9\n",
            "input: [[60, 27, 26, 11, 4, 18], [58, 27, 14, 3, 5, 18], [61, 27, 14, 3, 5, 18], [65, 27, 14, 3, 5, 18], [57, 27, 14, 15, 5, 18], [61, 27, 14, 15, 5, 18], [65, 27, 14, 15, 5, 18], [58, 27, 18, 27, 5, 18], [61, 27, 18, 27, 5, 18], [66, 27, 18, 27, 5, 18]]\n",
            "target: [[58, 27, 14, 3, 5, 18], [61, 27, 14, 3, 5, 18], [65, 27, 14, 3, 5, 18], [57, 27, 14, 15, 5, 18], [61, 27, 14, 15, 5, 18], [65, 27, 14, 15, 5, 18], [58, 27, 18, 27, 5, 18], [61, 27, 18, 27, 5, 18], [66, 27, 18, 27, 5, 18], [57, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 10\n",
            "input: [[54, 27, 14, 3, 3, 18], [59, 27, 14, 3, 3, 18], [62, 27, 14, 3, 3, 18], [54, 27, 14, 15, 3, 18], [58, 27, 14, 15, 3, 18], [62, 27, 14, 15, 3, 18], [55, 27, 18, 27, 3, 18], [59, 27, 18, 27, 3, 18], [62, 27, 18, 27, 3, 18], [52, 27, 26, 11, 4, 18]]\n",
            "target: [[59, 27, 14, 3, 3, 18], [62, 27, 14, 3, 3, 18], [54, 27, 14, 15, 3, 18], [58, 27, 14, 15, 3, 18], [62, 27, 14, 15, 3, 18], [55, 27, 18, 27, 3, 18], [59, 27, 18, 27, 3, 18], [62, 27, 18, 27, 3, 18], [52, 27, 26, 11, 4, 18], [58, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 11\n",
            "input: [[61, 27, 26, 11, 4, 18], [59, 27, 14, 3, 5, 18], [62, 27, 14, 3, 5, 18], [66, 27, 14, 3, 5, 18], [58, 27, 14, 15, 5, 18], [62, 27, 14, 15, 5, 18], [66, 27, 14, 15, 5, 18], [59, 27, 18, 27, 5, 18], [62, 27, 18, 27, 5, 18], [67, 27, 18, 27, 5, 18]]\n",
            "target: [[59, 27, 14, 3, 5, 18], [62, 27, 14, 3, 5, 18], [66, 27, 14, 3, 5, 18], [58, 27, 14, 15, 5, 18], [62, 27, 14, 15, 5, 18], [66, 27, 14, 15, 5, 18], [59, 27, 18, 27, 5, 18], [62, 27, 18, 27, 5, 18], [67, 27, 18, 27, 5, 18], [58, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 12\n",
            "input: [[56, 27, 14, 3, 3, 18], [61, 27, 14, 3, 3, 18], [64, 27, 14, 3, 3, 18], [56, 27, 14, 15, 3, 18], [60, 27, 14, 15, 3, 18], [64, 27, 14, 15, 3, 18], [57, 27, 18, 27, 3, 18], [61, 27, 18, 27, 3, 18], [64, 27, 18, 27, 3, 18], [54, 27, 26, 11, 4, 18]]\n",
            "target: [[61, 27, 14, 3, 3, 18], [64, 27, 14, 3, 3, 18], [56, 27, 14, 15, 3, 18], [60, 27, 14, 15, 3, 18], [64, 27, 14, 15, 3, 18], [57, 27, 18, 27, 3, 18], [61, 27, 18, 27, 3, 18], [64, 27, 18, 27, 3, 18], [54, 27, 26, 11, 4, 18], [60, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 13\n",
            "input: [[63, 27, 26, 11, 4, 18], [61, 27, 14, 3, 5, 18], [64, 27, 14, 3, 5, 18], [68, 27, 14, 3, 5, 18], [60, 27, 14, 15, 5, 18], [64, 27, 14, 15, 5, 18], [68, 27, 14, 15, 5, 18], [61, 27, 18, 27, 5, 18], [64, 27, 18, 27, 5, 18], [69, 27, 18, 27, 5, 18]]\n",
            "target: [[61, 27, 14, 3, 5, 18], [64, 27, 14, 3, 5, 18], [68, 27, 14, 3, 5, 18], [60, 27, 14, 15, 5, 18], [64, 27, 14, 15, 5, 18], [68, 27, 14, 15, 5, 18], [61, 27, 18, 27, 5, 18], [64, 27, 18, 27, 5, 18], [69, 27, 18, 27, 5, 18], [60, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 14\n",
            "input: [[55, 27, 14, 3, 3, 18], [60, 27, 14, 3, 3, 18], [63, 27, 14, 3, 3, 18], [55, 27, 14, 15, 3, 18], [59, 27, 14, 15, 3, 18], [63, 27, 14, 15, 3, 18], [56, 27, 18, 27, 3, 18], [60, 27, 18, 27, 3, 18], [63, 27, 18, 27, 3, 18], [53, 27, 26, 11, 4, 18]]\n",
            "target: [[60, 27, 14, 3, 3, 18], [63, 27, 14, 3, 3, 18], [55, 27, 14, 15, 3, 18], [59, 27, 14, 15, 3, 18], [63, 27, 14, 15, 3, 18], [56, 27, 18, 27, 3, 18], [60, 27, 18, 27, 3, 18], [63, 27, 18, 27, 3, 18], [53, 27, 26, 11, 4, 18], [59, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 15\n",
            "input: [[62, 27, 26, 11, 4, 18], [60, 27, 14, 3, 5, 18], [63, 27, 14, 3, 5, 18], [67, 27, 14, 3, 5, 18], [59, 27, 14, 15, 5, 18], [63, 27, 14, 15, 5, 18], [67, 27, 14, 15, 5, 18], [60, 27, 18, 27, 5, 18], [63, 27, 18, 27, 5, 18], [68, 27, 18, 27, 5, 18]]\n",
            "target: [[60, 27, 14, 3, 5, 18], [63, 27, 14, 3, 5, 18], [67, 27, 14, 3, 5, 18], [59, 27, 14, 15, 5, 18], [63, 27, 14, 15, 5, 18], [67, 27, 14, 15, 5, 18], [60, 27, 18, 27, 5, 18], [63, 27, 18, 27, 5, 18], [68, 27, 18, 27, 5, 18], [59, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 16\n",
            "input: [[57, 27, 14, 3, 3, 18], [62, 27, 14, 3, 3, 18], [65, 27, 14, 3, 3, 18], [57, 27, 14, 15, 3, 18], [61, 27, 14, 15, 3, 18], [65, 27, 14, 15, 3, 18], [58, 27, 18, 27, 3, 18], [62, 27, 18, 27, 3, 18], [65, 27, 18, 27, 3, 18], [55, 27, 26, 11, 4, 18]]\n",
            "target: [[62, 27, 14, 3, 3, 18], [65, 27, 14, 3, 3, 18], [57, 27, 14, 15, 3, 18], [61, 27, 14, 15, 3, 18], [65, 27, 14, 15, 3, 18], [58, 27, 18, 27, 3, 18], [62, 27, 18, 27, 3, 18], [65, 27, 18, 27, 3, 18], [55, 27, 26, 11, 4, 18], [61, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 17\n",
            "input: [[64, 27, 26, 11, 4, 18], [62, 27, 14, 3, 5, 18], [65, 27, 14, 3, 5, 18], [69, 27, 14, 3, 5, 18], [61, 27, 14, 15, 5, 18], [65, 27, 14, 15, 5, 18], [69, 27, 14, 15, 5, 18], [62, 27, 18, 27, 5, 18], [65, 27, 18, 27, 5, 18], [70, 27, 18, 27, 5, 18]]\n",
            "target: [[62, 27, 14, 3, 5, 18], [65, 27, 14, 3, 5, 18], [69, 27, 14, 3, 5, 18], [61, 27, 14, 15, 5, 18], [65, 27, 14, 15, 5, 18], [69, 27, 14, 15, 5, 18], [62, 27, 18, 27, 5, 18], [65, 27, 18, 27, 5, 18], [70, 27, 18, 27, 5, 18], [61, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 18\n",
            "input: [[58, 27, 14, 3, 3, 18], [63, 27, 14, 3, 3, 18], [66, 27, 14, 3, 3, 18], [58, 27, 14, 15, 3, 18], [62, 27, 14, 15, 3, 18], [66, 27, 14, 15, 3, 18], [59, 27, 18, 27, 3, 18], [63, 27, 18, 27, 3, 18], [66, 27, 18, 27, 3, 18], [56, 27, 26, 11, 4, 18]]\n",
            "target: [[63, 27, 14, 3, 3, 18], [66, 27, 14, 3, 3, 18], [58, 27, 14, 15, 3, 18], [62, 27, 14, 15, 3, 18], [66, 27, 14, 15, 3, 18], [59, 27, 18, 27, 3, 18], [63, 27, 18, 27, 3, 18], [66, 27, 18, 27, 3, 18], [56, 27, 26, 11, 4, 18], [62, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 19\n",
            "input: [[65, 27, 26, 11, 4, 18], [63, 27, 14, 3, 5, 18], [66, 27, 14, 3, 5, 18], [70, 27, 14, 3, 5, 18], [62, 27, 14, 15, 5, 18], [66, 27, 14, 15, 5, 18], [70, 27, 14, 15, 5, 18], [63, 27, 18, 27, 5, 18], [66, 27, 18, 27, 5, 18], [71, 27, 18, 27, 5, 18]]\n",
            "target: [[63, 27, 14, 3, 5, 18], [66, 27, 14, 3, 5, 18], [70, 27, 14, 3, 5, 18], [62, 27, 14, 15, 5, 18], [66, 27, 14, 15, 5, 18], [70, 27, 14, 15, 5, 18], [63, 27, 18, 27, 5, 18], [66, 27, 18, 27, 5, 18], [71, 27, 18, 27, 5, 18], [62, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 20\n",
            "input: [[59, 27, 14, 3, 3, 18], [64, 27, 14, 3, 3, 18], [67, 27, 14, 3, 3, 18], [59, 27, 14, 15, 3, 18], [63, 27, 14, 15, 3, 18], [67, 27, 14, 15, 3, 18], [60, 27, 18, 27, 3, 18], [64, 27, 18, 27, 3, 18], [67, 27, 18, 27, 3, 18], [57, 27, 26, 11, 4, 18]]\n",
            "target: [[64, 27, 14, 3, 3, 18], [67, 27, 14, 3, 3, 18], [59, 27, 14, 15, 3, 18], [63, 27, 14, 15, 3, 18], [67, 27, 14, 15, 3, 18], [60, 27, 18, 27, 3, 18], [64, 27, 18, 27, 3, 18], [67, 27, 18, 27, 3, 18], [57, 27, 26, 11, 4, 18], [63, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 21\n",
            "input: [[66, 27, 26, 11, 4, 18], [64, 27, 14, 3, 5, 18], [67, 27, 14, 3, 5, 18], [71, 27, 14, 3, 5, 18], [63, 27, 14, 15, 5, 18], [67, 27, 14, 15, 5, 18], [71, 27, 14, 15, 5, 18], [64, 27, 18, 27, 5, 18], [67, 27, 18, 27, 5, 18], [72, 27, 18, 27, 5, 18]]\n",
            "target: [[64, 27, 14, 3, 5, 18], [67, 27, 14, 3, 5, 18], [71, 27, 14, 3, 5, 18], [63, 27, 14, 15, 5, 18], [67, 27, 14, 15, 5, 18], [71, 27, 14, 15, 5, 18], [64, 27, 18, 27, 5, 18], [67, 27, 18, 27, 5, 18], [72, 27, 18, 27, 5, 18], [63, 27, 26, 11, 6, 18]]\n",
            "-----\n",
            "batch number: 22\n",
            "input: [[60, 27, 14, 3, 3, 18], [65, 27, 14, 3, 3, 18], [68, 27, 14, 3, 3, 18], [60, 27, 14, 15, 3, 18], [64, 27, 14, 15, 3, 18], [68, 27, 14, 15, 3, 18], [61, 27, 18, 27, 3, 18], [65, 27, 18, 27, 3, 18], [68, 27, 18, 27, 3, 18], [58, 27, 26, 11, 4, 18]]\n",
            "target: [[65, 27, 14, 3, 3, 18], [68, 27, 14, 3, 3, 18], [60, 27, 14, 15, 3, 18], [64, 27, 14, 15, 3, 18], [68, 27, 14, 15, 3, 18], [61, 27, 18, 27, 3, 18], [65, 27, 18, 27, 3, 18], [68, 27, 18, 27, 3, 18], [58, 27, 26, 11, 4, 18], [64, 27, 26, 11, 4, 18]]\n",
            "-----\n",
            "batch number: 23\n",
            "input: [[67, 27, 26, 11, 4, 18], [65, 27, 14, 3, 5, 18], [68, 27, 14, 3, 5, 18], [72, 27, 14, 3, 5, 18], [64, 27, 14, 15, 5, 18], [68, 27, 14, 15, 5, 18], [72, 27, 14, 15, 5, 18], [65, 27, 18, 27, 5, 18], [68, 27, 18, 27, 5, 18], [73, 27, 18, 27, 5, 18]]\n",
            "target: [[65, 27, 14, 3, 5, 18], [68, 27, 14, 3, 5, 18], [72, 27, 14, 3, 5, 18], [64, 27, 14, 15, 5, 18], [68, 27, 14, 15, 5, 18], [72, 27, 14, 15, 5, 18], [65, 27, 18, 27, 5, 18], [68, 27, 18, 27, 5, 18], [73, 27, 18, 27, 5, 18], [64, 27, 26, 11, 6, 18]]\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "directory = \"/content/gdrive/MyDrive/dummy/tokens/\"\n",
        "\n",
        "for fname in os.listdir(directory):                     # for each file in the directory\n",
        "    with open(os.path.join(directory, fname)) as i:     # open the file\n",
        "        data = json.load(i)                             # load the json file \n",
        "\n",
        "    for i in range(0, len(data['ids'][0])-1, 11):    # for each token in the file\n",
        "            x = data['ids'][0][i:i+attn_window]      # get the next 10 tokens\n",
        "            y = data['ids'][0][i+1:i+attn_window+1]  # get the next 1+10 tokens\n",
        "\n",
        "            if len(x) == attn_window and len(y) == attn_window: # if the length of the tokens is 10\n",
        "                \n",
        "                print('batch number:', j)            # print the batch number\n",
        "                print('input:', x)                   \n",
        "                print('target:', y) \n",
        "                print('-----')\n",
        "\n",
        "                j += 1                               # increment the batch number\n",
        "\n",
        "                input_seq.append(x)                  # append the input sequence\n",
        "                target_seq.append(y)                 # append the target sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7ULJHwh-7n-",
        "outputId": "682c62af-4318-4af7-b053-19e2877997b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_seq length: 24\n",
            "target_seq length: 24\n"
          ]
        }
      ],
      "source": [
        "print('input_seq length:', len(input_seq))\n",
        "print('target_seq length:', len(target_seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-AgZ0Mj-7n_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_size = 0.8 # 80/20 split between train and test+val data\n",
        "test_size = 0.5 # 50/50 (80/10/10) split between train, test and val\n",
        "\n",
        "x_train, x_rem, y_train, y_rem = train_test_split(input_seq, target_seq, train_size = 0.8)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_rem, y_rem, test_size = 0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjrsz0qR-7n_",
        "outputId": "13095116-cc5b-4159-910b-36c3cec48a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train input: 19\n",
            "train target: 19\n",
            "val input: 3\n",
            "val target: 3\n",
            "test input: 2\n",
            "test target: 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print('train input:', len(x_train)), print('train target:', len(y_train))\n",
        "print('val input:', len(x_val)), print('val target:', len(y_val))\n",
        "print('test input:', len(x_test)), print('test target:', len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "743BqedM-7oA"
      },
      "outputs": [],
      "source": [
        "\n",
        "#creates tokenizer and loads in parameters\n",
        "tokenizer = OctupleMono(params = '/content/gdrive/MyDrive/dummy/config.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrsENqqx-7oB"
      },
      "outputs": [],
      "source": [
        "# Creates model\n",
        "config = GPT2Config( \n",
        "    vocab_size=1000,\n",
        "    n_positions=attn_window,\n",
        "    n_embd=64,\n",
        "    n_layer=4,\n",
        "    n_head=4,\n",
        "    n_inner=128,\n",
        "    resid_pdrop=.1,\n",
        "    embd_pdrop=.1,\n",
        "    attn_pdrop=.1,\n",
        "    padding_token_id=tokenizer[0, 'PAD_None'],\n",
        "    bos_token_id=tokenizer[0, 'BOS_None'],\n",
        "    eos_token_id=tokenizer[0, 'EOS_None']\n",
        ")\n",
        "model = TFGPT2LMHeadModel(config)                      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpVCc1kD-7oB"
      },
      "outputs": [],
      "source": [
        "# defining our optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "# definining our loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# defining our metric which we want to observe\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "# compiling the model\n",
        "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq1q71gh-7oC",
        "outputId": "a118bbd2-905f-4bd2-8317-495157fd75ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 22s 22s/step - loss: 6.6270 - accuracy: 0.2719 - val_loss: 6.5895 - val_accuracy: 0.4389\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 209ms/step - loss: 6.6048 - accuracy: 0.3263 - val_loss: 6.5656 - val_accuracy: 0.5167\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 6.5816 - accuracy: 0.4132 - val_loss: 6.5429 - val_accuracy: 0.5889\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 6.5614 - accuracy: 0.4833 - val_loss: 6.5215 - val_accuracy: 0.6389\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 6.5477 - accuracy: 0.5456 - val_loss: 6.5013 - val_accuracy: 0.6833\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 6.5276 - accuracy: 0.5930 - val_loss: 6.4823 - val_accuracy: 0.6833\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 6.5113 - accuracy: 0.6404 - val_loss: 6.4644 - val_accuracy: 0.6833\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 6.4901 - accuracy: 0.6509 - val_loss: 6.4475 - val_accuracy: 0.6944\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 6.4787 - accuracy: 0.6596 - val_loss: 6.4315 - val_accuracy: 0.6944\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 191ms/step - loss: 6.4607 - accuracy: 0.6675 - val_loss: 6.4162 - val_accuracy: 0.6944\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 6.4474 - accuracy: 0.6772 - val_loss: 6.4016 - val_accuracy: 0.7000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 6.4357 - accuracy: 0.6825 - val_loss: 6.3875 - val_accuracy: 0.7000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 6.4184 - accuracy: 0.6772 - val_loss: 6.3739 - val_accuracy: 0.6944\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 189ms/step - loss: 6.4073 - accuracy: 0.6816 - val_loss: 6.3608 - val_accuracy: 0.6944\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 6.3958 - accuracy: 0.6842 - val_loss: 6.3479 - val_accuracy: 0.6944\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 6.3833 - accuracy: 0.6833 - val_loss: 6.3355 - val_accuracy: 0.6944\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 178ms/step - loss: 6.3752 - accuracy: 0.6947 - val_loss: 6.3234 - val_accuracy: 0.6944\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 169ms/step - loss: 6.3625 - accuracy: 0.6912 - val_loss: 6.3116 - val_accuracy: 0.6944\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 6.3541 - accuracy: 0.6956 - val_loss: 6.3002 - val_accuracy: 0.6944\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 6.3414 - accuracy: 0.6956 - val_loss: 6.2891 - val_accuracy: 0.6944\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 6.3288 - accuracy: 0.6939 - val_loss: 6.2784 - val_accuracy: 0.6889\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 6.3185 - accuracy: 0.6947 - val_loss: 6.2681 - val_accuracy: 0.6889\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 220ms/step - loss: 6.3100 - accuracy: 0.6947 - val_loss: 6.2582 - val_accuracy: 0.6889\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 215ms/step - loss: 6.3029 - accuracy: 0.6930 - val_loss: 6.2486 - val_accuracy: 0.6889\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 238ms/step - loss: 6.2937 - accuracy: 0.6965 - val_loss: 6.2395 - val_accuracy: 0.6889\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 6.2822 - accuracy: 0.6956 - val_loss: 6.2306 - val_accuracy: 0.6889\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 6.2760 - accuracy: 0.6982 - val_loss: 6.2222 - val_accuracy: 0.6889\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 236ms/step - loss: 6.2694 - accuracy: 0.7000 - val_loss: 6.2140 - val_accuracy: 0.6889\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 212ms/step - loss: 6.2602 - accuracy: 0.6982 - val_loss: 6.2062 - val_accuracy: 0.6889\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 218ms/step - loss: 6.2540 - accuracy: 0.6974 - val_loss: 6.1986 - val_accuracy: 0.6889\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 6.2453 - accuracy: 0.6956 - val_loss: 6.1912 - val_accuracy: 0.6889\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 216ms/step - loss: 6.2383 - accuracy: 0.7035 - val_loss: 6.1842 - val_accuracy: 0.6889\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 6.2295 - accuracy: 0.7018 - val_loss: 6.1773 - val_accuracy: 0.6889\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 239ms/step - loss: 6.2220 - accuracy: 0.6991 - val_loss: 6.1707 - val_accuracy: 0.6889\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 269ms/step - loss: 6.2159 - accuracy: 0.7009 - val_loss: 6.1642 - val_accuracy: 0.6889\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 6.2066 - accuracy: 0.6921 - val_loss: 6.1580 - val_accuracy: 0.6889\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 235ms/step - loss: 6.2035 - accuracy: 0.6974 - val_loss: 6.1519 - val_accuracy: 0.6889\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 223ms/step - loss: 6.1988 - accuracy: 0.6947 - val_loss: 6.1459 - val_accuracy: 0.6889\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 6.1918 - accuracy: 0.6947 - val_loss: 6.1401 - val_accuracy: 0.6889\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 171ms/step - loss: 6.1846 - accuracy: 0.6982 - val_loss: 6.1344 - val_accuracy: 0.6889\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 6.1811 - accuracy: 0.6991 - val_loss: 6.1288 - val_accuracy: 0.6889\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 6.1732 - accuracy: 0.7053 - val_loss: 6.1233 - val_accuracy: 0.6889\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 6.1704 - accuracy: 0.6930 - val_loss: 6.1179 - val_accuracy: 0.6889\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 6.1616 - accuracy: 0.7000 - val_loss: 6.1126 - val_accuracy: 0.6889\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 6.1574 - accuracy: 0.6939 - val_loss: 6.1073 - val_accuracy: 0.6889\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 162ms/step - loss: 6.1515 - accuracy: 0.6965 - val_loss: 6.1021 - val_accuracy: 0.6889\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 6.1437 - accuracy: 0.6947 - val_loss: 6.0970 - val_accuracy: 0.6889\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 6.1417 - accuracy: 0.6991 - val_loss: 6.0919 - val_accuracy: 0.6889\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 166ms/step - loss: 6.1387 - accuracy: 0.6991 - val_loss: 6.0869 - val_accuracy: 0.6889\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 6.1293 - accuracy: 0.7026 - val_loss: 6.0819 - val_accuracy: 0.6944\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 6.1257 - accuracy: 0.6965 - val_loss: 6.0771 - val_accuracy: 0.6944\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 168ms/step - loss: 6.1193 - accuracy: 0.7044 - val_loss: 6.0722 - val_accuracy: 0.6944\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 156ms/step - loss: 6.1145 - accuracy: 0.7035 - val_loss: 6.0675 - val_accuracy: 0.6944\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 182ms/step - loss: 6.1114 - accuracy: 0.6974 - val_loss: 6.0628 - val_accuracy: 0.6889\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 180ms/step - loss: 6.1079 - accuracy: 0.6991 - val_loss: 6.0582 - val_accuracy: 0.6889\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 6.1027 - accuracy: 0.7009 - val_loss: 6.0536 - val_accuracy: 0.6889\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 183ms/step - loss: 6.0950 - accuracy: 0.7018 - val_loss: 6.0492 - val_accuracy: 0.6889\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 6.0908 - accuracy: 0.7035 - val_loss: 6.0448 - val_accuracy: 0.6889\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 6.0870 - accuracy: 0.6982 - val_loss: 6.0404 - val_accuracy: 0.6889\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 6.0825 - accuracy: 0.7018 - val_loss: 6.0361 - val_accuracy: 0.6944\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 6.0767 - accuracy: 0.7018 - val_loss: 6.0319 - val_accuracy: 0.6944\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 6.0755 - accuracy: 0.7018 - val_loss: 6.0277 - val_accuracy: 0.6944\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 6.0723 - accuracy: 0.7053 - val_loss: 6.0236 - val_accuracy: 0.6944\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 6.0681 - accuracy: 0.7079 - val_loss: 6.0195 - val_accuracy: 0.6944\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 6.0611 - accuracy: 0.7000 - val_loss: 6.0155 - val_accuracy: 0.6944\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 6.0577 - accuracy: 0.7009 - val_loss: 6.0115 - val_accuracy: 0.7000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 6.0540 - accuracy: 0.7009 - val_loss: 6.0076 - val_accuracy: 0.7000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 6.0514 - accuracy: 0.6947 - val_loss: 6.0037 - val_accuracy: 0.7000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 6.0479 - accuracy: 0.6965 - val_loss: 5.9998 - val_accuracy: 0.6944\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 6.0399 - accuracy: 0.7079 - val_loss: 5.9960 - val_accuracy: 0.6944\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 6.0384 - accuracy: 0.6921 - val_loss: 5.9922 - val_accuracy: 0.6944\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 6.0344 - accuracy: 0.6991 - val_loss: 5.9884 - val_accuracy: 0.6944\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 188ms/step - loss: 6.0271 - accuracy: 0.7026 - val_loss: 5.9846 - val_accuracy: 0.6944\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 175ms/step - loss: 6.0258 - accuracy: 0.7061 - val_loss: 5.9808 - val_accuracy: 0.6944\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 160ms/step - loss: 6.0224 - accuracy: 0.7061 - val_loss: 5.9771 - val_accuracy: 0.6944\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 196ms/step - loss: 6.0184 - accuracy: 0.7018 - val_loss: 5.9734 - val_accuracy: 0.6944\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 172ms/step - loss: 6.0132 - accuracy: 0.7000 - val_loss: 5.9698 - val_accuracy: 0.6944\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 6.0117 - accuracy: 0.7044 - val_loss: 5.9662 - val_accuracy: 0.6944\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 6.0054 - accuracy: 0.7018 - val_loss: 5.9626 - val_accuracy: 0.6944\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 6.0070 - accuracy: 0.7000 - val_loss: 5.9590 - val_accuracy: 0.6944\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 5.9993 - accuracy: 0.7000 - val_loss: 5.9555 - val_accuracy: 0.6944\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 219ms/step - loss: 5.9954 - accuracy: 0.7018 - val_loss: 5.9520 - val_accuracy: 0.6889\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 221ms/step - loss: 5.9915 - accuracy: 0.7035 - val_loss: 5.9485 - val_accuracy: 0.6889\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 5.9875 - accuracy: 0.7018 - val_loss: 5.9451 - val_accuracy: 0.6889\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 208ms/step - loss: 5.9828 - accuracy: 0.7088 - val_loss: 5.9417 - val_accuracy: 0.6889\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 242ms/step - loss: 5.9829 - accuracy: 0.6982 - val_loss: 5.9383 - val_accuracy: 0.6889\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 237ms/step - loss: 5.9776 - accuracy: 0.6982 - val_loss: 5.9349 - val_accuracy: 0.6889\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 217ms/step - loss: 5.9751 - accuracy: 0.6991 - val_loss: 5.9315 - val_accuracy: 0.6889\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 233ms/step - loss: 5.9721 - accuracy: 0.7009 - val_loss: 5.9282 - val_accuracy: 0.6889\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 244ms/step - loss: 5.9667 - accuracy: 0.7044 - val_loss: 5.9249 - val_accuracy: 0.6944\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 234ms/step - loss: 5.9622 - accuracy: 0.6965 - val_loss: 5.9216 - val_accuracy: 0.7000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 263ms/step - loss: 5.9610 - accuracy: 0.7009 - val_loss: 5.9184 - val_accuracy: 0.6944\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 5.9579 - accuracy: 0.6982 - val_loss: 5.9151 - val_accuracy: 0.6944\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 227ms/step - loss: 5.9530 - accuracy: 0.7044 - val_loss: 5.9119 - val_accuracy: 0.6944\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 241ms/step - loss: 5.9493 - accuracy: 0.7114 - val_loss: 5.9086 - val_accuracy: 0.6944\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 161ms/step - loss: 5.9477 - accuracy: 0.7035 - val_loss: 5.9054 - val_accuracy: 0.6944\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 165ms/step - loss: 5.9423 - accuracy: 0.7079 - val_loss: 5.9022 - val_accuracy: 0.6889\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 5.9408 - accuracy: 0.7018 - val_loss: 5.8990 - val_accuracy: 0.6889\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 5.9381 - accuracy: 0.7096 - val_loss: 5.8958 - val_accuracy: 0.6889\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 164ms/step - loss: 5.9351 - accuracy: 0.7000 - val_loss: 5.8926 - val_accuracy: 0.6889\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 100\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = [\n",
        "\n",
        "# tf.keras.callbacks.ModelCheckpoint(\"/content/gdrive/MyDrive/models_1/\",\n",
        "#                                                  verbose=1)\n",
        "\n",
        "tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: model.save_pretrained(\"/content/gdrive/MyDrive/dummy/OCTMONO1/\"))\n",
        "]\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=num_epoch, callbacks=[cp_callback], validation_data = (x_val, y_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uckx2kjCnCrk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "65556ce0ff34c38326a7d49d831b93e01b4970706282dcba94d6ca8895201e9b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}